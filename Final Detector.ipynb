{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2815d37-0921-4fef-b72b-1e8e7f06a3b7",
   "metadata": {},
   "source": [
    "# ◈ Setting Up Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336309e-b6b2-4d98-9df2-2d91551b3177",
   "metadata": {},
   "source": [
    "### AIM: To install, import and connect the required python libraries & APIs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee690a-c5ec-491d-8d19-9f197eac4ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Installing Tensorflow to build a LSTM neural network\n",
    "\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816c3cf-9a08-4da0-a321-bd7e1a1988f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing openCV to access webcam for data collection and predictions\n",
    "\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e6a38a-24a3-4ade-b907-492c0380233b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connecting to mediapipe API for landmarks\n",
    "\n",
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935c1df-94f9-4836-9c2d-522797201680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLYING NECESSARY IMPORTS\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5bab9-41d2-4c90-9d69-c140cd81b45c",
   "metadata": {},
   "source": [
    "# ◈ Accessing Keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f82d79-4b92-46d6-b91e-faa7c37c079c",
   "metadata": {},
   "source": [
    "### AIM: To obtain the information about facial expressions and bodily gestures using Mediapipe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094dfddd-abfd-44bc-83b6-10f4f87d4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING OBJECT OF THE HOLISTIC MODEL & \n",
    "\n",
    "mp_holistic = mp.solutions.holistic         #Holistic Model\n",
    "mp_drawing = mp.solutions.drawing_utils     #Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e8283-3a08-4f34-884f-a5e52382cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mideapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)     #BGR to RGB color conversion\n",
    "    image.flags.writeable = False                      #Image not writeable\n",
    "    results = model.process(image)                     #Using mediapipe to make detections\n",
    "    image.flags.writeable = True                       #Image writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)     #RGB to BGR color conversion\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9283dff-3033-4cfb-ae4e-cee5912e4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION WAS THE KEY TO GET NUMERIC DATA FROM THE IMAGES ACCESSED USING OpenCV\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    #Draw Face Connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,\n",
    "                              mp_drawing.DrawingSpec(color = (80, 110, 10), thickness = 1, circle_radius = 1),\n",
    "                              mp_drawing.DrawingSpec(color = (80, 256, 121), thickness = 1, circle_radius = 1)\n",
    "                             )   \n",
    "    #Draw Pose Connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color = (80, 22, 10), thickness = 2, circle_radius = 2),\n",
    "                              mp_drawing.DrawingSpec(color = (80, 44, 121), thickness = 2, circle_radius = 1)\n",
    "                             ) \n",
    "    #Draw Left Hand Connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color = (121, 22, 76), thickness = 2, circle_radius = 2),\n",
    "                              mp_drawing.DrawingSpec(color = (121, 44, 250), thickness = 2, circle_radius = 1)\n",
    "                             )  \n",
    "    #Draw Right Hand Connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color = (121, 22, 76), thickness = 2, circle_radius = 2),\n",
    "                              mp_drawing.DrawingSpec(color = (121, 44, 250), thickness = 2, circle_radius = 1)\n",
    "                             )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9cbac2-cf31-4b37-8b2a-0d980415a6c0",
   "metadata": {},
   "source": [
    "#### The code snippet below was used as a trial to see the landmarks and features being marked in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea34b0-f3b2-4e81-8ac3-669fad104c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)   \n",
    "                        #0 works for window webcams, you can try 1 & 2 according to the OS and webcam type (External or Internal)\n",
    "\n",
    "#Using mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "    \n",
    "        #Read Feed\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        #Make Detections\n",
    "        image, results = mideapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        #Draw Landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "    \n",
    "        #Show to Screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    \n",
    "        #Break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91c243-91fc-425c-a787-bd069218575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORCEFULLY RELEASING THE WEBCAM\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404838d-75e6-4556-bf45-5e8ee3bd9cfe",
   "metadata": {},
   "source": [
    "# ◈ Keypoint Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089c52e8-501a-4863-93d3-9df693696011",
   "metadata": {},
   "source": [
    "### AIM: Extracting the landmarks processed earlier into useable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bfb8a9-cc9c-4b71-8075-2feb1bd0431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION BELOW ACCESSES THE COORDINATES OF THE LANDMARKS AND TURNS THEM INTO A 1 DIMENSIONAL NUMPY ARRAY\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379c31ca-20eb-404f-bd2d-2e6beaee04f8",
   "metadata": {},
   "source": [
    "# ◈ Directory Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309b667-782c-4c90-824e-358450b13142",
   "metadata": {},
   "source": [
    "### AIM: Connecting of the internal file directories so as to create a new folder for data collection and storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4486128-10b4-4750-ae91-f18a8e675988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A PIPELINE TO THE LOCAL DIRECTORY & ADDING DATA SPECIFICATIONS\n",
    "\n",
    "# Using os module in python for easy access of the local directories and wide range usability\n",
    "DATA_PATH = os.path.join('MP_Data')\n",
    "\n",
    "# Specifying the actions to be accomodated in the project & defining its scope\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "\n",
    "# It is the number of videos (sequence) to be collected for each action\n",
    "no_sequences = 30\n",
    "\n",
    "# This defines the number of frames each video (sequence) will have\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19e550-a469-462e-8c6a-8b5c5a19614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A COMPLETE SUB-DIRECTORY TREE WITHIN THE DIRECTORY\n",
    "\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73150b1d-33b4-4288-8603-c93a21b24b9b",
   "metadata": {},
   "source": [
    "#### Data strorage directory named 'MP_Data's structure (after running the above cell)\n",
    "\n",
    "```\n",
    "MP_Data/\n",
    "│\n",
    "├── hello/\n",
    "│   ├── 0/\n",
    "│   ├── 1/\n",
    "│   ├── 2/\n",
    "│   ├── ...\n",
    "│   ├── 29/\n",
    "│\n",
    "├── thanks/\n",
    "│   ├── 0/\n",
    "│   ├── 1/\n",
    "│   ├── 2/\n",
    "│   ├── ...\n",
    "│   ├── 29/\n",
    "│\n",
    "├── iloveyou/\n",
    "│   ├── 0/\n",
    "│   ├── 1/\n",
    "│   ├── 2/\n",
    "│   ├── ...\n",
    "│   ├── 29/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f4e23-015d-4931-852a-4dfb24b15b69",
   "metadata": {},
   "source": [
    "# ◈ Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cec53d1-b20d-4ad9-a4ad-5bad1b307bce",
   "metadata": {},
   "source": [
    "### AIM: Collecting keypoint values for training & testing and thier storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502dd3b-f822-44a9-8f4c-62ee24467c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# USED TO COLLECT DATA (VIDEOS OR SEQUENCES) WITHIN THE ABOVE CREATE FOLDERS\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Using mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "\n",
    "    #Loop through actions\n",
    "    for action in actions:\n",
    "        #Loop through sequences aka videos\n",
    "        for sequence in range(no_sequences):\n",
    "            #Loop through video length aka frame\n",
    "            for frame_num in range(sequence_length):\n",
    "                \n",
    "                #Read Feed\n",
    "                ret, frame = cap.read()\n",
    "            \n",
    "                #Make Detections\n",
    "                image, results = mideapipe_detection(frame, holistic)\n",
    "                print(results)\n",
    "        \n",
    "                #Draw Landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "\n",
    "                #Apply collection logic\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING_COLLECTION', (120, 200),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15, 12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.waitKey(3000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15, 12),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                #Export Keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "            \n",
    "                #Show to Screen\n",
    "                cv2.imshow('OpenCV Feed', image)\n",
    "            \n",
    "                #Break\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    #Releases the webcam automatically as soon as all the directories are visited for successful data storage\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe381c61-c3a8-4092-958b-de057249f778",
   "metadata": {},
   "source": [
    "#### Directory structure after data collection & storage\n",
    "\n",
    "```\n",
    "MP_Data/\n",
    "│\n",
    "├── hello/\n",
    "│   ├── 0/\n",
    "│   │   ├── 0.npy\n",
    "│   │   ├── 1.npy\n",
    "│   │   ├── ...\n",
    "│   │   ├── 29.npy\n",
    "│   ├── 1/\n",
    "│   ├── 2/\n",
    "│   ├── ...\n",
    "│   ├── 29/\n",
    "│\n",
    "├── thanks/\n",
    "│   ├── 0/\n",
    "│   │   ├── 0.npy\n",
    "│   │   ├── 1.npy\n",
    "│   │   ├── ...\n",
    "│   │   ├── 29.npy\n",
    "│   ├── 1/\n",
    "│   ├── 2/\n",
    "│   ├── ...\n",
    "│   ├── 29/\n",
    "│\n",
    "├── iloveyou/\n",
    "│   ├── 0/\n",
    "│   │   ├── 0.npy\n",
    "│   │   ├── 1.npy\n",
    "│   │   ├── ...\n",
    "│   │   ├── 29.npy\n",
    "│   ├── 1/\n",
    "│   ├── 2/\n",
    "│   ├── ...\n",
    "│   ├── 29/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660f94e4-8391-469e-9d09-db813bbe8eff",
   "metadata": {},
   "source": [
    "# ◈ Data Preprocessing & Label + Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e405fb4f-15ee-4b34-a44a-3d904532b3fe",
   "metadata": {},
   "source": [
    "### AIM: To preprocess & analyze the data and making it training and testing worthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01755c8d-7f84-42cb-9ec1-6859ced48acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split    #To split the data into training and testing sets\n",
    "from tensorflow.keras.utils import to_categorical       #One-Hot Ended formatting to labels (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f374eb8-9714-451e-8617-704b3c2b0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING A LABEL MAP OF THE PRE-DEFINED ACTIONS\n",
    "\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2c3c9-e37b-4de8-be4e-8424dd9a8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING IN THE COLLECTED DATA\n",
    "\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781b169-323c-4a10-a230-9c51ce2fe74d",
   "metadata": {},
   "source": [
    "#### Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced7e196-2bf0-42ca-8e9e-24d7ee5176b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e718c19-a5c8-4a19-a13c-9ca1a301d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa946b2-63be-492f-a79a-eed274e1a38f",
   "metadata": {},
   "source": [
    "#### Splitting data & analyzing the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4fb20-0cf4-4d54-95c9-fbfaf8e1a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547b798-57ab-40e3-952c-6276f3fb913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8404e5a-717a-403e-81a2-864555ff3982",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a83a8-8b24-4ada-8be6-4f3e6dfcecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34feb27b-7925-4345-8aab-281b6e04fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb64e5-05b9-4fa2-b631-802ac4a2dad3",
   "metadata": {},
   "source": [
    "# ◈ Model Building & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa01048-2ece-4e0a-8964-dca698e62c48",
   "metadata": {},
   "source": [
    "### AIM: Using tensorflow & keras for model building and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca046d78-8389-4eef-9a09-bd06fafef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential          #Leveraging the sequential model under keras\n",
    "from tensorflow.keras.layers import LSTM, Dense         #Importing the layers to be used in the model\n",
    "from tensorflow.keras.callbacks import TensorBoard      #Importing tensorboard for easy visualizations of model's metrics\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540e75e7-e5cc-49cd-9e7f-34b813de5acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP THE 'Logs' DIRECTORY FOR TENSORBOARD INITIALISATION\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir = log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d01b3a-ab14-47fb-b9df-92d42994facb",
   "metadata": {},
   "source": [
    "#### Steps to access TensorBoard\n",
    "\n",
    "1.**Open Command Prompt in the Logs Directory**  \n",
    "-Navigate to the directory where your TensorBoard logs are stored.  \n",
    "-If you are inside Jupyter Notebook, you can find the logs directory using:  \n",
    "```python\n",
    "import os\n",
    "print(os.getcwd())  # This prints the current working directory\n",
    "```  \n",
    "-Open **Command Prompt (cmd)** and navigate to the logs folder using:  \n",
    "```\n",
    "cd path\\to\\logs\\directory\n",
    "```  \n",
    "\n",
    "2.**Run TensorBoard**  \n",
    "-In the command prompt, type the following command:  \n",
    "```\n",
    "tensorboard --logdir=logs --port=6006\n",
    "```  \n",
    "-Replace `logs` with the actual path to your log directory if needed.  \n",
    "\n",
    "3.**Access TensorBoard in Browser**  \n",
    "-After running the command, TensorBoard will start and provide a URL like:  \n",
    "```\n",
    "http://localhost:6006/\n",
    "```  \n",
    "-Open this URL in your web browser to view TensorBoard.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5599dcd-6d23-4369-b79c-e95e0ea736ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A NEURAL NETWORK IS BUILD IN THE SEQUENTIAL MODEL WITH LSTM & DENSE LAYERS HAVING APPROPRIATE NEURONS\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences = True, activation = 'relu', input_shape = (30, 1662)))\n",
    "model.add(LSTM(128, return_sequences = True, activation = 'relu'))\n",
    "model.add(LSTM(64, return_sequences = False, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(actions.shape[0], activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23747a76-8cde-46b3-ae30-4dc6385f2bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPILATION IS CARRIED OUT\n",
    "\n",
    "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb2c1da-71e8-4f99-97a4-f6f2a9f4a14f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MODEL IS FIT OVER THE TRAINING DATASETS (SUPERVISED LEARNING AS OUTPUT LABELS WILL ALSO BE PROVIDED)\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 1000, callbacks = [tb_callback])   \n",
    "                            # NOTE: Stop the epochs as soon as the desired accuracy is achieved to avoid overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6074b-4dec-4eda-8bda-204b71fe8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A GENERAL SUMMARISATION OF MODEL AND ITS PARAMETERS\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0b0fff-50c9-4579-bbae-d4931dfa6e53",
   "metadata": {},
   "source": [
    "# ◈ Save Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af405e9a-9db9-4ab8-8957-131f781e6654",
   "metadata": {},
   "source": [
    "### AIM: Saving the model & its parameters for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2e5fdc-651c-4683-9330-0622e3efa9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model\n",
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcd25b-c4c8-44df-a83e-ea6574741056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Deletion of model\n",
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7744e-5cd6-4712-828f-b4683a3cf660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading weights into a newly created neural network and bypassing the time used for fitting it over training data\n",
    "model.load_weights('action.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e7ca2-66fd-457a-bcfe-f3d859fe2385",
   "metadata": {},
   "source": [
    "# ◈ Test Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562fa5e-d3b5-46a1-86c7-d5a71c6a1c82",
   "metadata": {},
   "source": [
    "### AIM: Making predictions over testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda6ddd3-7664-4c55-9fba-ec8cadca7397",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8316d-0cd3-48fe-95e2-af574264fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc918d-1ce0-4fd0-9459-b99c4b5aff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a118b3-63b2-45a0-bbb3-fbe0e749c821",
   "metadata": {},
   "source": [
    "# ◈ Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e6efa-1b29-4bbd-86df-735abb452f65",
   "metadata": {},
   "source": [
    "### AIM: Evaluating model using confusion matrix & accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308801e-c944-4a68-8353-555886cf5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba54d59-c3f4-4962-a783-ee36c96eea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f972c434-96cb-4807-bf79-42a4441bffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis = 1).tolist()\n",
    "yhat = np.argmax(yhat, axis = 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6e8c57-8041-40ac-acb5-84d8706e5bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)    #Visualising the apt predictions of True Negatives and True Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c06f032-6bb3-40f3-b703-b81e62c70640",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)     #Gaining a metrics of the model predictions in comparison to the actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da13efb-a7ce-4268-ad4a-32da15aa11c5",
   "metadata": {},
   "source": [
    "# ◈ Real Time Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae087e0f-7418-46d1-a48d-f7c4f14a829d",
   "metadata": {},
   "source": [
    "### AIM: Achieving the final goal by making real time detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758abaeb-839b-40eb-b978-f8d0f54b5cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WE CAN START MAKING REAL TIME PREDICTIONS BY HAVING THE WEBCAM CAPTURE OUR GESTURES\n",
    "        #THROUGH THE FUNCTION BELOW, WE CAN SEE THE PREDICTIONS (INTERPRETATIONS OF SIGN LANGUAGE) ON THE TOP OF THE SCREEN\n",
    "\n",
    "#Detection Variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.995  #Can be tuned according to the requirement\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Using mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "    \n",
    "        #Read Feed\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        #Make Detections\n",
    "        image, results = mideapipe_detection(frame, holistic)\n",
    "        print(results)\n",
    "\n",
    "        #Draw Landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "\n",
    "        #Prediction Logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.insert(0, keypoints)\n",
    "        sequence = sequence[:30]\n",
    "\n",
    "        thres = res[np.argmax(res)]\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis = 0))[0]\n",
    "            print(thres)\n",
    "            \n",
    "        #Visualization Logic\n",
    "        if thres > threshold:\n",
    "            if len(sentence) > 0:\n",
    "                if actions[np.argmax(res)] != sentence[-1]:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "            else:\n",
    "                sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "        if len(sentence) > 5:\n",
    "            sentence = sentence[-5:]\n",
    "\n",
    "        cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "        #Show to Screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "    \n",
    "        #Break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11103e-8d67-4668-b4b2-8f25cca98a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELEASING THE WEB CAM AFTER SUCCESSFUL INTERPRETATIONS & SEAMLESS COMMUNICATION\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
